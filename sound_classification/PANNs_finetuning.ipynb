{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxo9KHZhSO8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b314cd0f-398a-4afb-d171-a45ee2a905ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4FCCEf3SHQr",
        "outputId": "d1f2f447-97c5-427d-c5ae-2c0a40049a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Aluminum': 0, 'Ceramic': 1, 'Plastic': 2, 'Paper': 3, 'Wood': 4}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "BASE_DIR = Path(\"/content/drive/MyDrive/robot_project/train_data\")\n",
        "\n",
        "# 5개 재질과 라벨 매핑\n",
        "MATERIALS = [\"Aluminum\", \"Ceramic\", \"Plastic\", \"Paper\", \"Wood\"]\n",
        "material_to_label = {m: i for i, m in enumerate(MATERIALS)}\n",
        "print(material_to_label)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing match to PANNs input type\n",
        "\n",
        "Find peak and seperate into 1 sec length audio files\n"
      ],
      "metadata": {
        "id": "NFIjbU1bw_Dj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvlQxnhcq7i4",
        "outputId": "f49f5bed-5ef1-4654-988a-f4ea89b75f22",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting noisereduce\n",
            "  Downloading noisereduce-3.0.3-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from noisereduce) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from noisereduce) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from noisereduce) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from noisereduce) (4.67.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from noisereduce) (1.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->noisereduce) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->noisereduce) (1.17.0)\n",
            "Downloading noisereduce-3.0.3-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: noisereduce\n",
            "Successfully installed noisereduce-3.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install noisereduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2gyGEXFSR7o"
      },
      "outputs": [],
      "source": [
        "import noisereduce as nr\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "def load_audio(file_path, sampling_rate=44100):\n",
        "    y, sampling_rate = librosa.load(file_path, sr=sampling_rate)\n",
        "    return y, sampling_rate\n",
        "\n",
        "def reduce_noise_spectral_gating(audio, sr=44100, prop_decrease=0.8, stationary=False):\n",
        "    return nr.reduce_noise(y=audio, sr=sr, stationary=stationary, prop_decrease=prop_decrease)\n",
        "\n",
        "def detect_impacts_peak_finding(audio, sr, height_ratio=0.3, min_distance_sec=0.5):\n",
        "    audio_abs = np.abs(audio)\n",
        "    max_amp = audio_abs.max() if len(audio_abs) > 0 else 0.0\n",
        "    if max_amp == 0:\n",
        "        return [], []\n",
        "\n",
        "    height = max_amp * height_ratio\n",
        "    distance = int(min_distance_sec * sr)\n",
        "\n",
        "    peaks, _ = find_peaks(audio_abs, height=height, distance=distance)\n",
        "    impact_indices = peaks.tolist()\n",
        "    impact_times = [idx / sr for idx in impact_indices]\n",
        "    return impact_times, impact_indices\n",
        "\n",
        "def trim_audio_around_impacts(\n",
        "    audio,\n",
        "    sr,\n",
        "    impact_times,\n",
        "    before_sec=0.05,\n",
        "    after_sec=0.95,\n",
        "    overlap_handling=\"exclude\",\n",
        "):\n",
        "    total_length_sec = before_sec + after_sec  # 1초\n",
        "    total_length_samples = int(total_length_sec * sr)\n",
        "    before_samples = int(before_sec * sr)\n",
        "    after_samples = int(after_sec * sr)\n",
        "\n",
        "    trimmed_segments = []\n",
        "\n",
        "    background_noise_mean = np.mean(audio[: int(0.5 * sr)]) if len(audio) > int(0.5 * sr) else 0.0\n",
        "\n",
        "    for impact_time in impact_times:\n",
        "        impact_idx = int(impact_time * sr)\n",
        "        start_idx = impact_idx - before_samples\n",
        "        end_idx = impact_idx + after_samples\n",
        "\n",
        "        # 1초 segment 만들기 (padding 포함)\n",
        "        if start_idx < 0:\n",
        "            segment = np.zeros(total_length_samples)\n",
        "            actual_start = 0\n",
        "            actual_end = min(end_idx, len(audio))\n",
        "            segment[-start_idx : -start_idx + (actual_end - actual_start)] = audio[actual_start:actual_end]\n",
        "        elif end_idx > len(audio):\n",
        "            segment = np.zeros(total_length_samples)\n",
        "            actual_start = max(start_idx, 0)\n",
        "            actual_end = len(audio)\n",
        "            segment[: actual_end - actual_start] = audio[actual_start:actual_end]\n",
        "        else:\n",
        "            segment = audio[start_idx:end_idx].copy()\n",
        "\n",
        "        # 다른 impact와 겹치는 경우 처리\n",
        "        segment_start_time = impact_time - before_sec\n",
        "        segment_end_time = impact_time + after_sec\n",
        "\n",
        "        has_overlap = False\n",
        "        for other_time in impact_times:\n",
        "            if other_time == impact_time:\n",
        "                continue\n",
        "            if segment_start_time <= other_time <= segment_end_time:\n",
        "                has_overlap = True\n",
        "                break\n",
        "\n",
        "        if has_overlap:\n",
        "            if overlap_handling == \"exclude\":\n",
        "                continue\n",
        "            elif overlap_handling == \"replace\":\n",
        "                for other_time in impact_times:\n",
        "                    if other_time == impact_time:\n",
        "                        continue\n",
        "                    if segment_start_time <= other_time <= segment_end_time:\n",
        "                        overlap_start_idx = int((other_time - segment_start_time) * sr) - int(0.05 * sr)\n",
        "                        overlap_end_idx = int((other_time - segment_start_time) * sr) + int(0.05 * sr)\n",
        "                        overlap_start_idx = max(0, overlap_start_idx)\n",
        "                        overlap_end_idx = min(len(segment), overlap_end_idx)\n",
        "                        segment[overlap_start_idx:overlap_end_idx] = background_noise_mean\n",
        "\n",
        "        trimmed_segments.append(segment)\n",
        "\n",
        "    return trimmed_segments\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Left5ajsddxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-IE_uvDSScZ"
      },
      "outputs": [],
      "source": [
        "def get_material_from_path(path: Path):\n",
        "    parts = path.parts\n",
        "    for m in MATERIALS:\n",
        "        if m in parts:\n",
        "            return m\n",
        "    return None  # 못 찾으면 None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FYdf05ESVe5",
        "outputId": "c24b4e66-d5e4-46a1-c2e5-e1d4532fc5d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 ogg 파일 수: 421\n",
            "총 ogx 파일 수: 210\n",
            "합계 audio 파일 수: 631\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 631/631 [10:17<00:00,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저장된 segment 개수: 2308\n",
            "메타데이터: /content/drive/MyDrive/robot_project/train_data/segments_metadata.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "SEGMENT_WAV_DIR = BASE_DIR / \"segments_wav\"  # /robot_project/segments_wav/<Material>/xxx.wav\n",
        "SEGMENT_WAV_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "meta = []\n",
        "\n",
        "# BASE_DIR 아래에 있는 모든 .ogg, .ogx를 한 번에 스캔\n",
        "ogg_files = list(BASE_DIR.rglob(\"*.ogg\"))\n",
        "ogx_files = list(BASE_DIR.rglob(\"*.ogx\"))\n",
        "audio_files = ogg_files + ogx_files\n",
        "\n",
        "print(\"총 ogg 파일 수:\", len(ogg_files))\n",
        "print(\"총 ogx 파일 수:\", len(ogx_files))\n",
        "print(\"합계 audio 파일 수:\", len(audio_files))\n",
        "\n",
        "global_idx = 0\n",
        "\n",
        "for audio_path in tqdm(audio_files):\n",
        "    material = get_material_from_path(audio_path)\n",
        "    if material is None:\n",
        "        # 재질 이름이 없는 경로면 스킵\n",
        "        continue\n",
        "    label = material_to_label[material]\n",
        "\n",
        "    try:\n",
        "        audio, sr = load_audio(str(audio_path), sampling_rate=44100)\n",
        "    except Exception as e:\n",
        "        print(\"로드 실패:\", audio_path, e)\n",
        "        continue\n",
        "\n",
        "    # 디노이즈\n",
        "    audio_denoised = reduce_noise_spectral_gating(audio, sr, prop_decrease=0.8) # prop_decrease 조절\n",
        "\n",
        "    # 피크 탐지\n",
        "    impact_times, impact_indices = detect_impacts_peak_finding(\n",
        "        audio_denoised,\n",
        "        sr,\n",
        "        height_ratio=0.3,\n",
        "        min_distance_sec=0.5,\n",
        "    )\n",
        "\n",
        "    if len(impact_times) == 0:\n",
        "        continue\n",
        "\n",
        "    # 충돌 주변 1초 클립 (overlap 있는 건 제외)\n",
        "    segments = trim_audio_around_impacts(\n",
        "        audio_denoised,\n",
        "        sr,\n",
        "        impact_times,\n",
        "        before_sec=0.05,\n",
        "        after_sec=0.95,\n",
        "        overlap_handling=\"exclude\",\n",
        "    )\n",
        "\n",
        "    # 각 segment를 32kHz로 리샘플하고 .wav 저장\n",
        "    for seg in segments:\n",
        "        seg_32k = librosa.resample(seg, orig_sr=sr, target_sr=32000)\n",
        "        seg_dir = SEGMENT_WAV_DIR / material\n",
        "        seg_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        global_idx += 1\n",
        "        out_path = seg_dir / f\"{global_idx:06d}.wav\"\n",
        "        sf.write(str(out_path), seg_32k, 32000)\n",
        "\n",
        "        meta.append({\n",
        "            \"segment_id\": global_idx,\n",
        "            \"wav_path\": str(out_path),\n",
        "            \"material\": material,\n",
        "            \"label\": label,\n",
        "            \"source_file\": str(audio_path),\n",
        "        })\n",
        "\n",
        "meta_df = pd.DataFrame(meta)\n",
        "meta_csv_path = BASE_DIR / \"segments_metadata.csv\"\n",
        "meta_df.to_csv(meta_csv_path, index=False)\n",
        "print(\"저장된 segment 개수:\", len(meta_df))\n",
        "print(\"메타데이터:\", meta_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Balanced Dataset\n",
        "각 class 당 동일한 개수의 train data로 학습\n",
        "\n",
        "- 'Aluminum'     293\n",
        "- 'Ceramic'   326\n",
        "- 'Glass'     *110*\n",
        "- 'Plastic'    1073\n",
        "- 'Paper'     333\n",
        "- 'Wood'    283\n",
        "\n",
        "Glass : 너무 적음. 그냥 빼는 것도 고려\n",
        "\n"
      ],
      "metadata": {
        "id": "kztg_uXXsqF2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TtgnKikp3Ww",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6857b80-10c2-43ea-afa8-aa56e039ba27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원래 클래스별 개수:\n",
            " label\n",
            "0     293\n",
            "1     326\n",
            "2    1073\n",
            "3     333\n",
            "4     283\n",
            "Name: count, dtype: int64\n",
            "각 클래스에서 샘플링할 개수: 283\n",
            "balanced 클래스별 개수:\n",
            " label\n",
            "0    283\n",
            "1    283\n",
            "2    283\n",
            "3    283\n",
            "4    283\n",
            "Name: count, dtype: int64\n",
            "balanced train csv 저장: /content/drive/MyDrive/robot_project/train_data/segments_train_emb_balanced.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1060895670.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.sample(n=min_count, random_state=42))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) 임베딩 메타데이터 불러오기\n",
        "csv_path = Path(BASE_DIR) / \"segments_metadata.csv\"\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# 2) label별 개수 확인\n",
        "class_counts = df[\"label\"].value_counts().sort_index()\n",
        "print(\"원래 클래스별 개수:\\n\", class_counts)\n",
        "\n",
        "# 3) 각 class에서 뽑을 개수 = 가장 적은 클래스의 개수\n",
        "min_count = class_counts.min()\n",
        "print(\"각 클래스에서 샘플링할 개수:\", min_count)\n",
        "\n",
        "# 4) 각 label 그룹에서 min_count개씩 랜덤 샘플링 → balanced df 생성\n",
        "balanced_df = (\n",
        "    df.groupby(\"label\", group_keys=False)\n",
        "      .apply(lambda g: g.sample(n=min_count, random_state=42))\n",
        "      .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 5) 섞어주기\n",
        "balanced_df = balanced_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"balanced 클래스별 개수:\\n\", balanced_df[\"label\"].value_counts().sort_index())\n",
        "\n",
        "# 6) 저장\n",
        "balanced_csv_path = Path(BASE_DIR) / \"segments_train_emb_balanced.csv\"\n",
        "balanced_df.to_csv(balanced_csv_path, index=False)\n",
        "print(\"balanced train csv 저장:\", balanced_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSX8PSWfp90U"
      },
      "source": [
        "# 1. PANNs Pretrained model as a feature extractor & Training Light Classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "# train set metadata root\n",
        "metadata_csv = Path(BASE_DIR) / \"segments_train_emb_balanced.csv\"\n",
        "train_df = pd.read_csv(metadata_csv)\n",
        "\n",
        "# label별 개수 확인\n",
        "class_counts = train_df[\"label\"].value_counts().sort_index()\n",
        "print(\"class_counts:\\n\", class_counts)\n"
      ],
      "metadata": {
        "id": "v2albypv9D8A",
        "outputId": "da7cfe4e-a673-4fdf-ddb3-819970321a92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class_counts:\n",
            " label\n",
            "0    283\n",
            "1    283\n",
            "2    283\n",
            "3    283\n",
            "4    283\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMDGIiZcp377",
        "outputId": "5659e3f7-7313-4eeb-bde5-875069b67231",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: panns-inference in /usr/local/lib/python3.12/dist-packages (0.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from panns-inference) (3.10.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from panns-inference) (0.11.0)\n",
            "Requirement already satisfied: torchlibrosa in /usr/local/lib/python3.12/dist-packages (from panns-inference) (0.1.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->panns-inference) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->panns-inference) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->panns-inference) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->panns-inference) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->panns-inference) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->panns-inference) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->panns-inference) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (2025.11.12)\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "GPU number: 1\n",
            "DataParallel\n"
          ]
        }
      ],
      "source": [
        "# PANNs Inference 사용\n",
        "!pip install panns-inference\n",
        "\n",
        "from panns_inference import AudioTagging\n",
        "import numpy as np\n",
        "\n",
        "at = AudioTagging(checkpoint_path=None, device='cuda')  # GPU 사용\n",
        "print(at.model.__class__.__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn0oYw-uqHpv"
      },
      "outputs": [],
      "source": [
        "def get_panns_embedding_from_wav(wav_path, target_sr=32000):\n",
        "    audio, sr = librosa.load(wav_path, sr=target_sr)\n",
        "    assert sr == target_sr, f\"sr mismatch: {sr}\"\n",
        "    audio = audio[None, :].astype(np.float32)  # (1, T)\n",
        "\n",
        "    # panns_inference의 inference API\n",
        "    clipwise_output, embedding = at.inference(audio)\n",
        "    # embedding shape: (1, D)\n",
        "    return embedding[0]  # (D,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h78fAPkkqJcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb267c42-22a9-416b-847d-066a507d53e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 5/1415 [00:00<00:32, 43.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding dim: 2048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1415/1415 [05:16<00:00,  4.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "업데이트된 메타: /content/drive/MyDrive/robot_project/train_data/segments_train_emb_balanced.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# PANNs Output Embedding\n",
        "EMB_DIR = BASE_DIR / \"segments_embedding\"\n",
        "EMB_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "meta_df = pd.read_csv(metadata_csv)\n",
        "emb_dim = None\n",
        "\n",
        "emb_paths = []\n",
        "\n",
        "for i, row in tqdm(meta_df.iterrows(), total=len(meta_df)):\n",
        "    wav_path = row[\"wav_path\"]\n",
        "    segment_id = row[\"segment_id\"]\n",
        "\n",
        "    emb = get_panns_embedding_from_wav(wav_path, target_sr=32000) # get PANNs embedding from .wav file\n",
        "    if emb_dim is None:\n",
        "        emb_dim = emb.shape[0]\n",
        "        print(\"Embedding dim:\", emb_dim)\n",
        "\n",
        "    emb_path = EMB_DIR / f\"{segment_id:06d}.npy\" #.npy로 임베딩 저장\n",
        "    np.save(emb_path, emb.astype(np.float32))\n",
        "    emb_paths.append(str(emb_path))\n",
        "\n",
        "meta_df[\"emb_path\"] = emb_paths\n",
        "\n",
        "# csv file update\n",
        "meta_df.to_csv(metadata_csv, index=False)\n",
        "print(\"업데이트된 메타:\", metadata_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-b7bzbzqL3X"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ImpactEmbDataset(Dataset):\n",
        "    def __init__(self, csv_path):\n",
        "        import pandas as pd\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        emb = np.load(row[\"emb_path\"]).astype(np.float32)  # (D,)\n",
        "        label = int(row[\"label\"])\n",
        "        return torch.from_numpy(emb), torch.tensor(label, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRtpXNM5qPje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73260140-de42-48af-e948-99c7387b7b80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수: 1415\n",
            "전체 클래스 분포:\n",
            " label\n",
            "0    283\n",
            "1    283\n",
            "2    283\n",
            "3    283\n",
            "4    283\n",
            "Name: count, dtype: int64\n",
            "Train: 1130 Val: 285\n",
            "Train per class:\n",
            " label\n",
            "0    226\n",
            "1    226\n",
            "2    226\n",
            "3    226\n",
            "4    226\n",
            "Name: count, dtype: int64\n",
            "Val per class:\n",
            " label\n",
            "0    57\n",
            "1    57\n",
            "2    57\n",
            "3    57\n",
            "4    57\n",
            "Name: count, dtype: int64\n",
            "Train size: 1130\n",
            "Val size: 285\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "full_df = pd.read_csv(metadata_csv)\n",
        "\n",
        "print(\"전체 샘플 수:\", len(full_df))\n",
        "print(\"전체 클래스 분포:\\n\", full_df[\"label\"].value_counts().sort_index())\n",
        "\n",
        "train_ratio = 0.8\n",
        "train_parts = []\n",
        "val_parts = []\n",
        "\n",
        "# label별로 나눠서 각 클래스 안에서 8:2 split\n",
        "for label, group in full_df.groupby(\"label\"):\n",
        "    group = group.sample(frac=1.0, random_state=42).reset_index(drop=True)  # 그룹 내 셔플\n",
        "    n_total = len(group)\n",
        "    n_train = int(n_total * train_ratio)\n",
        "\n",
        "    train_part = group.iloc[:n_train].reset_index(drop=True)\n",
        "    val_part   = group.iloc[n_train:].reset_index(drop=True)\n",
        "\n",
        "    train_parts.append(train_part)\n",
        "    val_parts.append(val_part)\n",
        "\n",
        "# 클래스별로 나눠둔 걸 다시 합치고, 전체 한 번 더 셔플\n",
        "train_df = pd.concat(train_parts, axis=0).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "val_df   = pd.concat(val_parts,   axis=0).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Train:\", len(train_df), \"Val:\", len(val_df))\n",
        "print(\"Train per class:\\n\", train_df[\"label\"].value_counts().sort_index())\n",
        "print(\"Val per class:\\n\",   val_df[\"label\"].value_counts().sort_index())\n",
        "\n",
        "train_csv = BASE_DIR / \"segments_train_emb.csv\"\n",
        "val_csv   = BASE_DIR / \"segments_val_emb.csv\"\n",
        "train_df.to_csv(train_csv, index=False)\n",
        "val_df.to_csv(val_csv, index=False)\n",
        "\n",
        "train_dataset = ImpactEmbDataset(train_csv)\n",
        "val_dataset   = ImpactEmbDataset(val_csv)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "print(\"Train size:\", len(train_dataset))\n",
        "print(\"Val size:\", len(val_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_VPsrJhqR4E"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "EMB_DIM = emb_dim  # 앞에서 구한 embedding dimension\n",
        "\n",
        "class EmbClassifier(nn.Module):\n",
        "    def __init__(self, emb_dim=EMB_DIM, num_classes=5):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(emb_dim, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.3, training=self.training)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pa2F8yYqTyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45f7114b-e6ca-4363-ed01-5a9a440e4bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 Train loss: 0.9629, acc: 0.647 | Val loss: 1.1989, acc: 0.688\n",
            "Epoch 02 Train loss: 0.6034, acc: 0.796 | Val loss: 0.7059, acc: 0.789\n",
            "Epoch 03 Train loss: 0.4502, acc: 0.853 | Val loss: 0.5374, acc: 0.821\n",
            "Epoch 04 Train loss: 0.3795, acc: 0.879 | Val loss: 0.5007, acc: 0.839\n",
            "Epoch 05 Train loss: 0.3334, acc: 0.882 | Val loss: 0.4456, acc: 0.825\n",
            "Epoch 06 Train loss: 0.2779, acc: 0.906 | Val loss: 0.4256, acc: 0.856\n",
            "Epoch 07 Train loss: 0.2546, acc: 0.918 | Val loss: 0.4084, acc: 0.867\n",
            "Epoch 08 Train loss: 0.2058, acc: 0.930 | Val loss: 0.3693, acc: 0.874\n",
            "Epoch 09 Train loss: 0.1981, acc: 0.935 | Val loss: 0.5015, acc: 0.839\n",
            "Epoch 10 Train loss: 0.2015, acc: 0.933 | Val loss: 0.3912, acc: 0.863\n",
            "Epoch 11 Train loss: 0.1569, acc: 0.946 | Val loss: 0.4966, acc: 0.842\n",
            "Epoch 12 Train loss: 0.1543, acc: 0.942 | Val loss: 0.4351, acc: 0.863\n",
            "Epoch 13 Train loss: 0.1332, acc: 0.952 | Val loss: 0.3960, acc: 0.874\n",
            "Epoch 14 Train loss: 0.1190, acc: 0.964 | Val loss: 0.5100, acc: 0.849\n",
            "Epoch 15 Train loss: 0.1051, acc: 0.964 | Val loss: 0.5769, acc: 0.818\n",
            "Epoch 16 Train loss: 0.1104, acc: 0.958 | Val loss: 0.5628, acc: 0.835\n",
            "Epoch 17 Train loss: 0.1057, acc: 0.962 | Val loss: 0.6033, acc: 0.811\n",
            "Epoch 18 Train loss: 0.1189, acc: 0.960 | Val loss: 0.5041, acc: 0.828\n",
            "Epoch 19 Train loss: 0.0907, acc: 0.971 | Val loss: 0.4239, acc: 0.877\n",
            "Epoch 20 Train loss: 0.0921, acc: 0.960 | Val loss: 0.5548, acc: 0.853\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = EmbClassifier(emb_dim=EMB_DIM, num_classes=len(MATERIALS)).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ----- train -----\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for emb, labels in train_loader:\n",
        "        emb = emb.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(emb)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * labels.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_loss /= train_total\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # ----- validation -----\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for emb, labels in val_loader:\n",
        "            emb = emb.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(emb)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * labels.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} \"\n",
        "          f\"Train loss: {train_loss:.4f}, acc: {train_acc:.3f} | \"\n",
        "          f\"Val loss: {val_loss:.4f}, acc: {val_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1lKId7UqV6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "456d9f9c-771a-4ae9-eabc-334fa4876cb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "모델 저장: /content/drive/MyDrive/robot_project/train_data/panns_emb_classifier_20epoch.pth\n"
          ]
        }
      ],
      "source": [
        "SAVE_PATH = BASE_DIR / \"panns_emb_classifier_20epoch.pth\"\n",
        "torch.save({\n",
        "    \"model_state_dict\": model.state_dict(),\n",
        "    \"emb_dim\": EMB_DIM,\n",
        "    \"num_classes\": len(MATERIALS),\n",
        "    \"material_to_label\": material_to_label,\n",
        "}, SAVE_PATH)\n",
        "print(\"모델 저장:\", SAVE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = torch.load(SAVE_PATH, map_location=device)\n",
        "\n",
        "EMB_DIM = ckpt[\"emb_dim\"]\n",
        "num_classes = ckpt[\"num_classes\"]\n",
        "\n",
        "model = EmbClassifier(emb_dim=EMB_DIM, num_classes=num_classes).to(device)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "model.eval()  # ← 평가 모드\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5Xc2PYg09rj",
        "outputId": "788682d5-43f9-44da-d298-ed2ce3436d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EmbClassifier(\n",
              "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
              "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc3): Linear(in_features=256, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. PANNs Fine-Tuning with Balanced data"
      ],
      "metadata": {
        "id": "NlK3oIBBEPe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import librosa\n",
        "from pathlib import Path\n",
        "\n",
        "balanced_csv_path = Path(BASE_DIR) / \"segments_train_emb_balanced.csv\"\n",
        "full_df = pd.read_csv(balanced_csv_path)\n",
        "\n",
        "print(\"Balanced 전체 개수:\", len(full_df))\n",
        "print(full_df.head())\n"
      ],
      "metadata": {
        "id": "TJMrtQqwZ5hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69340ca9-1d79-452e-aae4-3a8790e03224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced 전체 개수: 1415\n",
            "   segment_id                                           wav_path  material  \\\n",
            "0        2129  /content/drive/MyDrive/robot_project/train_dat...   Plastic   \n",
            "1        1732  /content/drive/MyDrive/robot_project/train_dat...     Paper   \n",
            "2        1909  /content/drive/MyDrive/robot_project/train_dat...   Ceramic   \n",
            "3         218  /content/drive/MyDrive/robot_project/train_dat...  Aluminum   \n",
            "4         196  /content/drive/MyDrive/robot_project/train_dat...  Aluminum   \n",
            "\n",
            "   label                                        source_file  \\\n",
            "0      2  /content/drive/MyDrive/robot_project/train_dat...   \n",
            "1      3  /content/drive/MyDrive/robot_project/train_dat...   \n",
            "2      1  /content/drive/MyDrive/robot_project/train_dat...   \n",
            "3      0  /content/drive/MyDrive/robot_project/train_dat...   \n",
            "4      0  /content/drive/MyDrive/robot_project/train_dat...   \n",
            "\n",
            "                                            emb_path  \n",
            "0  /content/drive/MyDrive/robot_project/train_dat...  \n",
            "1  /content/drive/MyDrive/robot_project/train_dat...  \n",
            "2  /content/drive/MyDrive/robot_project/train_dat...  \n",
            "3  /content/drive/MyDrive/robot_project/train_dat...  \n",
            "4  /content/drive/MyDrive/robot_project/train_dat...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "full_df = full_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_parts = []\n",
        "val_parts = []\n",
        "train_ratio = 0.8\n",
        "\n",
        "for label, group in full_df.groupby(\"label\"):\n",
        "    group = group.sample(frac=1.0, random_state=42).reset_index(drop=True)  # 그룹 내 셔플\n",
        "    n_total = len(group)\n",
        "    n_train = int(n_total * train_ratio)\n",
        "\n",
        "    train_part = group.iloc[:n_train].reset_index(drop=True)\n",
        "    val_part   = group.iloc[n_train:].reset_index(drop=True)\n",
        "\n",
        "    train_parts.append(train_part)\n",
        "    val_parts.append(val_part)\n",
        "\n",
        "train_df = pd.concat(train_parts, axis=0).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "val_df   = pd.concat(val_parts,   axis=0).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Train:\", len(train_df), \"Val:\", len(val_df))\n",
        "print(\"Train per class:\\n\", train_df[\"label\"].value_counts().sort_index())\n",
        "print(\"Val per class:\\n\",   val_df[\"label\"].value_counts().sort_index())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chw1NOiNoQaR",
        "outputId": "95ff66c3-5233-4c6d-90af-9b6a95df8f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 1130 Val: 285\n",
            "Train per class:\n",
            " label\n",
            "0    226\n",
            "1    226\n",
            "2    226\n",
            "3    226\n",
            "4    226\n",
            "Name: count, dtype: int64\n",
            "Val per class:\n",
            " label\n",
            "0    57\n",
            "1    57\n",
            "2    57\n",
            "3    57\n",
            "4    57\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImpactWaveDataset(Dataset):\n",
        "    def __init__(self, df, target_sr=32000, target_len=32000, mode=\"train\"):\n",
        "        \"\"\"\n",
        "        df: DataFrame (columns: wav_path, label, ...)\n",
        "        target_sr: PANNs가 사용하는 32kHz\n",
        "        target_len: 파형 길이 (1초 = 32000 샘플). 부족하면 zero-pad, 길면 crop\n",
        "        mode: \"train\"이면 random crop, \"val\"이면 center crop\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.target_sr = target_sr\n",
        "        self.target_len = target_len\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _fix_length(self, wav: np.ndarray):\n",
        "        L = len(wav)\n",
        "        T = self.target_len\n",
        "        if L == T:\n",
        "            return wav\n",
        "        elif L < T:\n",
        "            # 뒤쪽 zero-padding\n",
        "            pad = T - L\n",
        "            return np.pad(wav, (0, pad), mode=\"constant\")\n",
        "        else:\n",
        "            # 길면 crop: train은 random, val은 center\n",
        "            if self.mode == \"train\":\n",
        "                start = np.random.randint(0, L - T + 1)\n",
        "            else:\n",
        "                start = max(0, (L - T) // 2)\n",
        "            return wav[start:start+T]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        wav_path = row[\"wav_path\"]\n",
        "        label = int(row[\"label\"])\n",
        "\n",
        "        # 32kHz로 로드\n",
        "        wav, sr = librosa.load(wav_path, sr=self.target_sr)\n",
        "        wav = self._fix_length(wav).astype(np.float32)\n",
        "\n",
        "        return torch.from_numpy(wav), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = ImpactWaveDataset(train_df, target_sr=32000, target_len=32000, mode=\"train\")\n",
        "val_dataset   = ImpactWaveDataset(val_df,   target_sr=32000, target_len=32000, mode=\"val\")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader), \"Val batches:\", len(val_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eG0RKaf1oSTp",
        "outputId": "b688e131-375e-4b12-9352-c34e90705ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 17 Val batches: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install panns-inference\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from panns_inference import AudioTagging\n",
        "from torchlibrosa.augmentation import SpecAugmentation # Add this import!\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "# Pretrained PANNs (예: Cnn14)\n",
        "at = AudioTagging(checkpoint_path=None, device=device)\n",
        "panns_model = at.model  # 이게 실제 torch.nn.Module (Cnn14 계열)\n",
        "print(panns_model.__class__.__name__)\n",
        "\n",
        "num_classes = len(MATERIALS)\n",
        "\n",
        "# 원래 AudioSet용 fc layer를 가져와서 in_features 확인\n",
        "in_features = panns_model.fc_audioset.in_features\n",
        "print(\"fc_audioset in_features:\", in_features)\n",
        "\n",
        "# Get original SpecAugmentation parameters (time_stripes_num, freq_drop_width, freq_stripes_num)\n",
        "# as they are not affected by this fix and should retain their default values.\n",
        "original_time_stripes_num = panns_model.spec_augmenter.time_dropper.stripes_num\n",
        "original_freq_drop_width = panns_model.spec_augmenter.freq_dropper.drop_width\n",
        "original_freq_stripes_num = panns_model.spec_augmenter.freq_dropper.stripes_num\n",
        "\n",
        "# Re-initialize SpecAugmentation with a smaller time_drop_width.\n",
        "# The spectrogram time dimension for a 1-second audio (32kHz, n_fft=2048, hop_length=1024) is approximately 30 frames.\n",
        "# The original time_drop_width was 64, which is too large and causes the RuntimeError.\n",
        "# Setting it to a value smaller than 30 (e.g., 20) will prevent the error.\n",
        "panns_model.spec_augmenter = SpecAugmentation(\n",
        "    time_drop_width=20, # Reduced from the problematic 64\n",
        "    time_stripes_num=original_time_stripes_num,\n",
        "    freq_drop_width=original_freq_drop_width,\n",
        "    freq_stripes_num=original_freq_stripes_num\n",
        ")\n",
        "\n",
        "# 새로운 5-class head로 교체\n",
        "panns_model.fc_audioset = nn.Linear(in_features, num_classes)\n",
        "\n",
        "panns_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5g_wQpK6oYsS",
        "outputId": "c93341c7-f911-4225-a9c2-74f23ce57719",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting panns-inference\n",
            "  Downloading panns_inference-0.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from panns-inference) (3.10.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (from panns-inference) (0.11.0)\n",
            "Collecting torchlibrosa (from panns-inference)\n",
            "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.5.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa->panns-inference) (1.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->panns-inference) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa->panns-inference) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->panns-inference) (4.5.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa->panns-inference) (2.32.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->panns-inference) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa->panns-inference) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa->panns-inference) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->panns-inference) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns-inference) (2025.11.12)\n",
            "Downloading panns_inference-0.1.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: torchlibrosa, panns-inference\n",
            "Successfully installed panns-inference-0.1.1 torchlibrosa-0.1.0\n",
            "device: cuda\n",
            "Checkpoint path: /root/panns_data/Cnn14_mAP=0.431.pth\n",
            "Using CPU.\n",
            "Cnn14\n",
            "fc_audioset in_features: 2048\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cnn14(\n",
              "  (spectrogram_extractor): Spectrogram(\n",
              "    (stft): STFT(\n",
              "      (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
              "      (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (logmel_extractor): LogmelFilterBank()\n",
              "  (spec_augmenter): SpecAugmentation(\n",
              "    (time_dropper): DropStripes()\n",
              "    (freq_dropper): DropStripes()\n",
              "  )\n",
              "  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_block1): ConvBlock(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block2): ConvBlock(\n",
              "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block3): ConvBlock(\n",
              "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block4): ConvBlock(\n",
              "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block5): ConvBlock(\n",
              "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block6): ConvBlock(\n",
              "    (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "  (fc_audioset): Linear(in_features=2048, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(panns_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # ---------- Train ----------\n",
        "    panns_model.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "\n",
        "    for batch_wav, labels in train_loader:\n",
        "        batch_wav = batch_wav.to(device)           # (B, T)\n",
        "        labels = labels.to(device)                 # (B,)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # PANNs forward\n",
        "        # Corrected: Access elements from the dictionary returned by panns_model\n",
        "        output_dict = panns_model(batch_wav)\n",
        "        clipwise_output = output_dict[\"clipwise_output\"]\n",
        "        embedding = output_dict[\"embedding\"]\n",
        "        # clipwise_output: (B, num_classes)\n",
        "\n",
        "        loss = criterion(clipwise_output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * labels.size(0)\n",
        "        _, preds = clipwise_output.max(1)\n",
        "        train_correct += (preds == labels).sum().item()\n",
        "        train_total += labels.size(0)\n",
        "\n",
        "    train_loss /= train_total\n",
        "    train_acc = train_correct / train_total\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    panns_model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_wav, labels in val_loader:\n",
        "            batch_wav = batch_wav.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Corrected: Access elements from the dictionary returned by panns_model\n",
        "            output_dict = panns_model(batch_wav)\n",
        "            clipwise_output = output_dict[\"clipwise_output\"]\n",
        "            embedding = output_dict[\"embedding\"]\n",
        "\n",
        "            loss = criterion(clipwise_output, labels)\n",
        "\n",
        "            val_loss += loss.item() * labels.size(0)\n",
        "            _, preds = clipwise_output.max(1)\n",
        "            val_correct += (preds == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "\n",
        "    val_loss /= val_total\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "    print(f\"[Epoch {epoch+1:02d}] \"\n",
        "          f\"Train loss: {train_loss:.4f}, acc: {train_acc:.3f} | \"\n",
        "          f\"Val loss: {val_loss:.4f}, acc: {val_acc:.3f}\")"
      ],
      "metadata": {
        "id": "fFgtQXXPoes5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "834bf328-41b9-49c9-88b1-53c071613dc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 01] Train loss: 1.6090, acc: 0.210 | Val loss: 1.6052, acc: 0.312\n",
            "[Epoch 02] Train loss: 1.6020, acc: 0.335 | Val loss: 1.5938, acc: 0.512\n",
            "[Epoch 03] Train loss: 1.5590, acc: 0.395 | Val loss: 1.5334, acc: 0.544\n",
            "[Epoch 04] Train loss: 1.4398, acc: 0.523 | Val loss: 1.3926, acc: 0.656\n",
            "[Epoch 05] Train loss: 1.3327, acc: 0.641 | Val loss: 1.2542, acc: 0.719\n",
            "[Epoch 06] Train loss: 1.2712, acc: 0.666 | Val loss: 1.1665, acc: 0.807\n",
            "[Epoch 07] Train loss: 1.2055, acc: 0.737 | Val loss: 1.1092, acc: 0.849\n",
            "[Epoch 08] Train loss: 1.1586, acc: 0.781 | Val loss: 1.0703, acc: 0.867\n",
            "[Epoch 09] Train loss: 1.1258, acc: 0.793 | Val loss: 1.0497, acc: 0.881\n",
            "[Epoch 10] Train loss: 1.1034, acc: 0.835 | Val loss: 1.0311, acc: 0.895\n",
            "[Epoch 11] Train loss: 1.0920, acc: 0.823 | Val loss: 1.0156, acc: 0.912\n",
            "[Epoch 12] Train loss: 1.0666, acc: 0.847 | Val loss: 1.0033, acc: 0.923\n",
            "[Epoch 13] Train loss: 1.0537, acc: 0.864 | Val loss: 0.9931, acc: 0.930\n",
            "[Epoch 14] Train loss: 1.0403, acc: 0.873 | Val loss: 0.9873, acc: 0.923\n",
            "[Epoch 15] Train loss: 1.0332, acc: 0.877 | Val loss: 0.9812, acc: 0.933\n",
            "[Epoch 16] Train loss: 1.0245, acc: 0.888 | Val loss: 0.9750, acc: 0.937\n",
            "[Epoch 17] Train loss: 1.0219, acc: 0.892 | Val loss: 0.9708, acc: 0.947\n",
            "[Epoch 18] Train loss: 1.0087, acc: 0.904 | Val loss: 0.9678, acc: 0.947\n",
            "[Epoch 19] Train loss: 1.0096, acc: 0.893 | Val loss: 0.9656, acc: 0.944\n",
            "[Epoch 20] Train loss: 0.9971, acc: 0.912 | Val loss: 0.9643, acc: 0.947\n",
            "[Epoch 21] Train loss: 0.9899, acc: 0.925 | Val loss: 0.9601, acc: 0.947\n",
            "[Epoch 22] Train loss: 0.9747, acc: 0.938 | Val loss: 0.9568, acc: 0.951\n",
            "[Epoch 23] Train loss: 0.9933, acc: 0.915 | Val loss: 0.9565, acc: 0.947\n",
            "[Epoch 24] Train loss: 0.9803, acc: 0.926 | Val loss: 0.9537, acc: 0.951\n",
            "[Epoch 25] Train loss: 0.9841, acc: 0.918 | Val loss: 0.9540, acc: 0.951\n",
            "[Epoch 26] Train loss: 0.9737, acc: 0.934 | Val loss: 0.9516, acc: 0.951\n",
            "[Epoch 27] Train loss: 0.9799, acc: 0.926 | Val loss: 0.9497, acc: 0.954\n",
            "[Epoch 28] Train loss: 0.9708, acc: 0.940 | Val loss: 0.9512, acc: 0.951\n",
            "[Epoch 29] Train loss: 0.9644, acc: 0.943 | Val loss: 0.9502, acc: 0.951\n",
            "[Epoch 30] Train loss: 0.9624, acc: 0.940 | Val loss: 0.9492, acc: 0.951\n",
            "[Epoch 31] Train loss: 0.9609, acc: 0.942 | Val loss: 0.9482, acc: 0.954\n",
            "[Epoch 32] Train loss: 0.9566, acc: 0.949 | Val loss: 0.9483, acc: 0.954\n",
            "[Epoch 33] Train loss: 0.9554, acc: 0.948 | Val loss: 0.9495, acc: 0.954\n",
            "[Epoch 34] Train loss: 0.9536, acc: 0.948 | Val loss: 0.9455, acc: 0.954\n",
            "[Epoch 35] Train loss: 0.9541, acc: 0.952 | Val loss: 0.9434, acc: 0.954\n",
            "[Epoch 36] Train loss: 0.9535, acc: 0.954 | Val loss: 0.9432, acc: 0.958\n",
            "[Epoch 37] Train loss: 0.9521, acc: 0.951 | Val loss: 0.9458, acc: 0.958\n",
            "[Epoch 38] Train loss: 0.9530, acc: 0.950 | Val loss: 0.9415, acc: 0.958\n",
            "[Epoch 39] Train loss: 0.9504, acc: 0.953 | Val loss: 0.9408, acc: 0.954\n",
            "[Epoch 40] Train loss: 0.9564, acc: 0.948 | Val loss: 0.9411, acc: 0.954\n",
            "[Epoch 41] Train loss: 0.9393, acc: 0.962 | Val loss: 0.9419, acc: 0.954\n",
            "[Epoch 42] Train loss: 0.9518, acc: 0.948 | Val loss: 0.9398, acc: 0.954\n",
            "[Epoch 43] Train loss: 0.9421, acc: 0.962 | Val loss: 0.9381, acc: 0.954\n",
            "[Epoch 44] Train loss: 0.9411, acc: 0.963 | Val loss: 0.9375, acc: 0.958\n",
            "[Epoch 45] Train loss: 0.9394, acc: 0.965 | Val loss: 0.9379, acc: 0.954\n",
            "[Epoch 46] Train loss: 0.9409, acc: 0.967 | Val loss: 0.9388, acc: 0.954\n",
            "[Epoch 47] Train loss: 0.9402, acc: 0.960 | Val loss: 0.9392, acc: 0.958\n",
            "[Epoch 48] Train loss: 0.9407, acc: 0.960 | Val loss: 0.9403, acc: 0.961\n",
            "[Epoch 49] Train loss: 0.9385, acc: 0.965 | Val loss: 0.9405, acc: 0.958\n",
            "[Epoch 50] Train loss: 0.9374, acc: 0.966 | Val loss: 0.9409, acc: 0.958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FT_SAVE_PATH = BASE_DIR / \"panns_finetuned_material5_50epoch.pth\"\n",
        "torch.save({\n",
        "    \"model_state_dict\": panns_model.state_dict(),\n",
        "    \"num_classes\": num_classes,\n",
        "    \"material_to_label\": material_to_label,\n",
        "}, FT_SAVE_PATH)\n",
        "print(\"Fine-tuned PANNs 저장:\", FT_SAVE_PATH)\n"
      ],
      "metadata": {
        "id": "eH5504R9orM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5486a76-0bd4-4297-ec6b-3d2f463dfb16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned PANNs 저장: /content/drive/MyDrive/robot_project/train_data/panns_finetuned_material5_50epoch.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Inr9wa0Cou17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "bO7d-M2HDzej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "\n",
        "def classify_impacts_in_wav(\n",
        "    wav_path,\n",
        "    model,\n",
        "    at,\n",
        "    device,\n",
        "    sr_orig=44100,\n",
        "    target_sr=32000,\n",
        "    height_ratio=0.3,\n",
        "    min_distance_sec=0.5,\n",
        "    before_sec=0.05,\n",
        "    after_sec=0.95,\n",
        "):\n",
        "    \"\"\"\n",
        "    하나의 .wav 파일에 대해:\n",
        "      - 충돌 시점들을 찾고\n",
        "      - 각 충돌 주변 1초 segment를 잘라서\n",
        "      - PANNs embedding + 학습된 분류기로 재질 예측\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    results: list of dict\n",
        "        [\n",
        "          {\n",
        "            \"impact_idx\": int,\n",
        "            \"impact_time\": float,\n",
        "            \"pred_label\": int,\n",
        "            \"pred_material\": str,\n",
        "            \"probs\": np.ndarray(num_classes,)\n",
        "          },\n",
        "          ...\n",
        "        ]\n",
        "    \"\"\"\n",
        "    wav_path = Path(wav_path)\n",
        "\n",
        "    # 1) 오디오 로드 + 디노이즈\n",
        "    audio, sr = load_audio(str(wav_path), sampling_rate=sr_orig)\n",
        "    audio_denoised = reduce_noise_spectral_gating(audio, sr, prop_decrease=0.8)\n",
        "\n",
        "    # 2) 피크 탐지\n",
        "    impact_times, impact_indices = detect_impacts_peak_finding(\n",
        "        audio_denoised,\n",
        "        sr,\n",
        "        height_ratio=height_ratio,\n",
        "        min_distance_sec=min_distance_sec,\n",
        "    )\n",
        "    if len(impact_times) == 0:\n",
        "        print(f\"[WARN] No impacts detected in {wav_path}\")\n",
        "        return []\n",
        "\n",
        "    # 3) 충돌 주변 1초 segment로 자르기 (overlap 제외)\n",
        "    segments = trim_audio_around_impacts(\n",
        "        audio_denoised,\n",
        "        sr,\n",
        "        impact_times,\n",
        "        before_sec=before_sec,\n",
        "        after_sec=after_sec,\n",
        "        overlap_handling=\"exclude\",\n",
        "    )\n",
        "\n",
        "    # impact_times와 segments 개수가 다를 수 있음 (overlap으로 제외된 것들 때문)\n",
        "    valid_times = []\n",
        "    for t in impact_times:\n",
        "        segment_start_time = t - before_sec\n",
        "        segment_end_time   = t + after_sec\n",
        "        has_overlap = False\n",
        "        for other_t in impact_times:\n",
        "            if other_t == t:\n",
        "                continue\n",
        "            if segment_start_time <= other_t <= segment_end_time:\n",
        "                has_overlap = True\n",
        "                break\n",
        "        if not has_overlap:\n",
        "            valid_times.append(t)\n",
        "\n",
        "    # 길이 체크\n",
        "    if len(segments) != len(valid_times):\n",
        "        min_len = min(len(segments), len(valid_times))\n",
        "        segments = segments[:min_len]\n",
        "        valid_times = valid_times[:min_len]\n",
        "\n",
        "    results = []\n",
        "    label_to_material = {v: k for k, v in material_to_label.items()}\n",
        "\n",
        "    # 4) 각 segment에 대해: resample → embedding → 분류\n",
        "    model.eval()\n",
        "    for i, (seg, t) in enumerate(zip(segments, valid_times)):\n",
        "        # 32kHz로 리샘플\n",
        "        seg_32k = librosa.resample(seg, orig_sr=sr, target_sr=target_sr)\n",
        "\n",
        "        # PANNs input: (1, T), float32\n",
        "        audio_in = seg_32k[None, :].astype(np.float32)\n",
        "        clipwise_output, embedding = at.inference(audio_in)  # embedding: (1, D)\n",
        "        emb = torch.from_numpy(embedding[0]).to(device)      # (D,)\n",
        "        emb = emb.unsqueeze(0)                               # (1, D)\n",
        "\n",
        "        # classifier 통과\n",
        "        with torch.no_grad():\n",
        "            logits = model(emb)            # (1, num_classes)\n",
        "            probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "            pred_label = int(np.argmax(probs))\n",
        "            pred_material = label_to_material[pred_label]\n",
        "\n",
        "        results.append({\n",
        "            \"impact_idx\": i,\n",
        "            \"impact_time\": float(t),\n",
        "            \"pred_label\": pred_label,\n",
        "            \"pred_material\": pred_material,\n",
        "            \"probs\": probs,\n",
        "        })\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "OwAKG-7d1CDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt = torch.load(FT_SAVE_PATH, map_location=device)\n",
        "\n",
        "panns_model = at.model   # 다시 base model 구조 가져오기\n",
        "in_features = panns_model.fc_audioset.in_features\n",
        "panns_model.fc_audioset = nn.Linear(in_features, ckpt[\"num_classes\"])\n",
        "panns_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "panns_model.to(device)\n",
        "panns_model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em67M25g0cb2",
        "outputId": "533d1b02-324c-4a3c-af51-c1eb0adc5851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Cnn14(\n",
              "  (spectrogram_extractor): Spectrogram(\n",
              "    (stft): STFT(\n",
              "      (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
              "      (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(320,), bias=False)\n",
              "    )\n",
              "  )\n",
              "  (logmel_extractor): LogmelFilterBank()\n",
              "  (spec_augmenter): SpecAugmentation(\n",
              "    (time_dropper): DropStripes()\n",
              "    (freq_dropper): DropStripes()\n",
              "  )\n",
              "  (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv_block1): ConvBlock(\n",
              "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block2): ConvBlock(\n",
              "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block3): ConvBlock(\n",
              "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block4): ConvBlock(\n",
              "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block5): ConvBlock(\n",
              "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (conv_block6): ConvBlock(\n",
              "    (conv1): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "  (fc_audioset): Linear(in_features=2048, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "label_to_material = {v: k for k, v in material_to_label.items()}\n",
        "\n",
        "def classify_impacts_in_wav_finetuned(\n",
        "    wav_path,\n",
        "    panns_model,\n",
        "    device,\n",
        "    sr_orig=44100,\n",
        "    target_sr=32000,\n",
        "    height_ratio=0.3,\n",
        "    min_distance_sec=0.5,\n",
        "    before_sec=0.05,\n",
        "    after_sec=0.95,\n",
        "):\n",
        "    wav_path = Path(wav_path)\n",
        "\n",
        "    # Helper function for length fixing, using center crop for inference\n",
        "    def _fix_length_for_inference(wav: np.ndarray, target_len: int):\n",
        "        L = len(wav)\n",
        "        T = target_len\n",
        "        if L == T:\n",
        "            return wav\n",
        "        elif L < T:\n",
        "            pad = T - L\n",
        "            return np.pad(wav, (0, pad), mode=\"constant\")\n",
        "        else:\n",
        "            start = max(0, (L - T) // 2)  # Center crop\n",
        "            return wav[start:start+T]\n",
        "\n",
        "    # 1) 오디오 로드 + 디노이즈\n",
        "    audio, sr = load_audio(str(wav_path), sampling_rate=sr_orig)\n",
        "    audio_denoised = reduce_noise_spectral_gating(audio, sr, prop_decrease=0.8)\n",
        "\n",
        "    # 2) 피크 탐지\n",
        "    impact_times, impact_indices = detect_impacts_peak_finding(\n",
        "        audio_denoised,\n",
        "        sr,\n",
        "        height_ratio=height_ratio,\n",
        "        min_distance_sec=min_distance_sec,\n",
        "    )\n",
        "    if len(impact_times) == 0:\n",
        "        print(f\"[WARN] No impacts detected in {wav_path}\")\n",
        "        return []\n",
        "\n",
        "    # 3) 충돌 주변 1초 segment로 자르기 (overlap 제외)\n",
        "    segments = trim_audio_around_impacts(\n",
        "        audio_denoised,\n",
        "        sr,\n",
        "        impact_times,\n",
        "        before_sec=before_sec,\n",
        "        after_sec=after_sec,\n",
        "        overlap_handling=\"exclude\",\n",
        "    )\n",
        "\n",
        "    # valid_times 계산 (overlap 없는 것만)\n",
        "    valid_times = []\n",
        "    for t in impact_times:\n",
        "        segment_start_time = t - before_sec\n",
        "        segment_end_time   = t + after_sec\n",
        "        has_overlap = False\n",
        "        for other_t in impact_times:\n",
        "            if other_t == t:\n",
        "                continue\n",
        "            if segment_start_time <= other_t <= segment_end_time:\n",
        "                has_overlap = True\n",
        "                break\n",
        "        if not has_overlap:\n",
        "            valid_times.append(t)\n",
        "\n",
        "    if len(segments) != len(valid_times):\n",
        "        min_len = min(len(segments), len(valid_times))\n",
        "        segments = segments[:min_len]\n",
        "        valid_times = valid_times[:min_len]\n",
        "\n",
        "    results = []\n",
        "    panns_model.eval()\n",
        "    target_len_samples = int(target_sr * (before_sec + after_sec)) # 1 second for 0.05 + 0.95\n",
        "\n",
        "    for i, (seg, t) in enumerate(zip(segments, valid_times)):\n",
        "        # 32kHz로 리샘플 & 길이 고정\n",
        "        seg_32k = librosa.resample(seg, orig_sr=sr, target_sr=target_sr)\n",
        "        seg_fixed = _fix_length_for_inference(seg_32k, target_len_samples)\n",
        "\n",
        "        wav_tensor = torch.from_numpy(seg_fixed.astype(np.float32)).unsqueeze(0).to(device)  # (1, T)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # PANNs fine-tuned model expects raw waveform\n",
        "            output_dict = panns_model(wav_tensor)\n",
        "            clipwise_output = output_dict[\"clipwise_output\"]\n",
        "            probs = F.softmax(clipwise_output, dim=1).cpu().numpy()[0]\n",
        "            pred_label = int(np.argmax(probs))\n",
        "            pred_material = label_to_material[pred_label]\n",
        "\n",
        "        results.append({\n",
        "            \"impact_idx\": i,\n",
        "            \"impact_time\": float(t),\n",
        "            \"pred_label\": pred_label,\n",
        "            \"pred_material\": pred_material,\n",
        "            \"probs\": probs,\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Ju-yW0Zlo3Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_file =  Path(\"/content/drive/MyDrive/robot_project\") /\"test_data\" / \"wood.wav\"  # wood, plastic, paper # 이거 test 파일이 좀 이상함 새로 녹음 부탁해야할듯\n",
        "\n",
        "results = classify_impacts_in_wav(test_file, model, at, device)\n",
        "\n",
        "for r in results[:10]:  # 앞 10개만 출력\n",
        "    print(f\"time={r['impact_time']:.3f}s, pred={r['pred_material']}, probs={np.round(r['probs'], 3)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WevhwVJe1GbG",
        "outputId": "f14e35ad-c12d-4446-ed5b-5c389e2c31a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time=0.296s, pred=Paper, probs=[0.003 0.025 0.008 0.864 0.1  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_file = Path(\"/content/drive/MyDrive/robot_project\") /\"test_data\" / \"paper.wav\"\n",
        "\n",
        "results = classify_impacts_in_wav_finetuned(\n",
        "    test_file,\n",
        "    panns_model,\n",
        "    device,\n",
        ")\n",
        "\n",
        "for r in results[:10]:\n",
        "    print(f\"time={r['impact_time']:.3f}s, pred={r['pred_material']}, probs={np.round(r['probs'], 3)}\")\n"
      ],
      "metadata": {
        "id": "cOt2w9cro54L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eceb5271-1e88-46bc-8290-ae77f66d5d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time=1.668s, pred=Aluminum, probs=[0.238 0.19  0.197 0.188 0.188]\n",
            "time=3.013s, pred=Ceramic, probs=[0.149 0.405 0.149 0.149 0.149]\n",
            "time=4.647s, pred=Ceramic, probs=[0.169 0.313 0.183 0.168 0.168]\n",
            "time=8.743s, pred=Plastic, probs=[0.142 0.189 0.385 0.142 0.142]\n",
            "time=10.414s, pred=Ceramic, probs=[0.13  0.353 0.256 0.13  0.13 ]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vSX8PSWfp90U"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}